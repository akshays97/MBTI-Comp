{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Myers Briggs Personality Type Classification - Abridged\n",
    "\n",
    "### Carolina Analytics and Data Science\n",
    "\n",
    "Data from [Kaggle](https://www.kaggle.com/datasnaek/mbti-type)\n",
    "\n",
    "The goal for this dataset will to be able to classify Myers Briggs Personality Type (MBTI) based on text someone with that personality type had written. In our meeting on Febuary 15 we will choose which \n",
    "\n",
    "This is starter code to get working on this dataset.\n",
    "\n",
    "Note there are two versions of this document. In this (the \"abridged\") we start with already featurized text that we have made here so that jumping right into the modelling is also an option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf # neural net library\n",
    "import numpy as np      # numerical/linear algebra\n",
    "import pandas as pd     # reading in data\n",
    "import re               # regular expression\n",
    "import wordcloud\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the CSV's generated in the \"full\" version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_training_featurized = pd.read_csv('data/featurized_train.csv', index_col='instance')\n",
    "featurized_testing = pd.read_csv('data/featurized_test.csv', index_col='instance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We decided in our meeting to predict Thinking and Feeling, so the third letter of the classification.\n",
    "\n",
    "Splitting into test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_training_featurized['Y'] = (1*(full_training_featurized['tf'] == 'T')) # having a Y column that is 0-1 valued\n",
    "\n",
    "np.random.seed(seed=1) # for consistency with test and training sets\n",
    "training_data, validation_data = np.split(full_training_featurized.sample(frac=1), [int(.75*len(full_training_featurized))]) # splitting into training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5394812680115274"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_x = training_data.drop(['ie', 'sn', 'tf', 'pj', 'Y'], axis=1)\n",
    "training_y = training_data['Y']\n",
    "\n",
    "validation_x = validation_data.drop(['ie', 'sn', 'tf', 'pj', 'Y'], axis=1)\n",
    "validation_y = validation_data['Y']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg = logreg.fit(training_x, training_y)\n",
    "\n",
    "logreg.score(training_x, training_y) # training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.454178674351585"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, validation_x, validation_y): \n",
    "    # testing accuracy \n",
    "    correct = 0\n",
    "    for (a, b) in zip(model.predict(validation_x), validation_y):\n",
    "        correct += (a-b)**2\n",
    "    return float(correct)/len(validation_y)\n",
    "\n",
    "evaluate_model(logreg, validation_x, validation_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98405379442843421"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest = random_forest.fit(training_x, training_y)\n",
    "\n",
    "random_forest.score(training_x, training_y) # training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44726224783861673"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(random_forest, validation_x, validation_y) # testing accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_training_y = np.array([training_y, -(training_y-1)]).T\n",
    "tf_validation_y = np.array([validation_y, -(validation_y-1)]).T\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 1000\n",
    "batch_size = 100\n",
    "display_step = 10\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 30 # 1st layer number of features\n",
    "n_hidden_2 = 20 # 2nd layer number of features\n",
    "n_input = 50 # Number of feature\n",
    "n_classes = 2 # Number of classes to predict\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.023678432\n",
      "Epoch: 0011 cost= 0.673817996\n",
      "Epoch: 0021 cost= 0.659029766\n",
      "Epoch: 0031 cost= 0.653910299\n",
      "Epoch: 0041 cost= 0.651840323\n",
      "Epoch: 0051 cost= 0.650847121\n",
      "Epoch: 0061 cost= 0.650112472\n",
      "Epoch: 0071 cost= 0.649528487\n",
      "Epoch: 0081 cost= 0.649032772\n",
      "Epoch: 0091 cost= 0.648469256\n",
      "Epoch: 0101 cost= 0.647889245\n",
      "Epoch: 0111 cost= 0.647322127\n",
      "Epoch: 0121 cost= 0.646470057\n",
      "Epoch: 0131 cost= 0.645499993\n",
      "Epoch: 0141 cost= 0.644632240\n",
      "Epoch: 0151 cost= 0.643568308\n",
      "Epoch: 0161 cost= 0.642698663\n",
      "Epoch: 0171 cost= 0.641924907\n",
      "Epoch: 0181 cost= 0.641100556\n",
      "Epoch: 0191 cost= 0.640194222\n",
      "Epoch: 0201 cost= 0.639317993\n",
      "Epoch: 0211 cost= 0.638550467\n",
      "Epoch: 0221 cost= 0.637822088\n",
      "Epoch: 0231 cost= 0.637159235\n",
      "Epoch: 0241 cost= 0.636580355\n",
      "Epoch: 0251 cost= 0.636020049\n",
      "Epoch: 0261 cost= 0.635442792\n",
      "Epoch: 0271 cost= 0.634940700\n",
      "Epoch: 0281 cost= 0.634449125\n",
      "Epoch: 0291 cost= 0.633768646\n",
      "Epoch: 0301 cost= 0.633194850\n",
      "Epoch: 0311 cost= 0.632712438\n",
      "Epoch: 0321 cost= 0.632157267\n",
      "Epoch: 0331 cost= 0.631622572\n",
      "Epoch: 0341 cost= 0.631056715\n",
      "Epoch: 0351 cost= 0.630578455\n",
      "Epoch: 0361 cost= 0.630092860\n",
      "Epoch: 0371 cost= 0.629682008\n",
      "Epoch: 0381 cost= 0.629247320\n",
      "Epoch: 0391 cost= 0.628763061\n",
      "Epoch: 0401 cost= 0.628370333\n",
      "Epoch: 0411 cost= 0.628083040\n",
      "Epoch: 0421 cost= 0.627680884\n",
      "Epoch: 0431 cost= 0.627162426\n",
      "Epoch: 0441 cost= 0.626693342\n",
      "Epoch: 0451 cost= 0.626340330\n",
      "Epoch: 0461 cost= 0.625942189\n",
      "Epoch: 0471 cost= 0.625576094\n",
      "Epoch: 0481 cost= 0.625302581\n",
      "Epoch: 0491 cost= 0.624968085\n",
      "Epoch: 0501 cost= 0.624621013\n",
      "Epoch: 0511 cost= 0.624410710\n",
      "Epoch: 0521 cost= 0.623996128\n",
      "Epoch: 0531 cost= 0.623928950\n",
      "Epoch: 0541 cost= 0.623510287\n",
      "Epoch: 0551 cost= 0.623130266\n",
      "Epoch: 0561 cost= 0.622865147\n",
      "Epoch: 0571 cost= 0.622759418\n",
      "Epoch: 0581 cost= 0.622248908\n",
      "Epoch: 0591 cost= 0.621932447\n",
      "Epoch: 0601 cost= 0.621678238\n",
      "Epoch: 0611 cost= 0.621398395\n",
      "Epoch: 0621 cost= 0.621019455\n",
      "Epoch: 0631 cost= 0.620724170\n",
      "Epoch: 0641 cost= 0.620326597\n",
      "Epoch: 0651 cost= 0.620118933\n",
      "Epoch: 0661 cost= 0.620068819\n",
      "Epoch: 0671 cost= 0.619734006\n",
      "Epoch: 0681 cost= 0.619762657\n",
      "Epoch: 0691 cost= 0.619276022\n",
      "Epoch: 0701 cost= 0.619064591\n",
      "Epoch: 0711 cost= 0.618534653\n",
      "Epoch: 0721 cost= 0.618151246\n",
      "Epoch: 0731 cost= 0.617920219\n",
      "Epoch: 0741 cost= 0.617603169\n",
      "Epoch: 0751 cost= 0.617345104\n",
      "Epoch: 0761 cost= 0.617005080\n",
      "Epoch: 0771 cost= 0.616793567\n",
      "Epoch: 0781 cost= 0.616154031\n",
      "Epoch: 0791 cost= 0.616056346\n",
      "Epoch: 0801 cost= 0.615900826\n",
      "Epoch: 0811 cost= 0.615666091\n",
      "Epoch: 0821 cost= 0.615409874\n",
      "Epoch: 0831 cost= 0.615034713\n",
      "Epoch: 0841 cost= 0.614689936\n",
      "Epoch: 0851 cost= 0.614366185\n",
      "Epoch: 0861 cost= 0.614073040\n",
      "Epoch: 0871 cost= 0.613828892\n",
      "Epoch: 0881 cost= 0.613667346\n",
      "Epoch: 0891 cost= 0.613486677\n",
      "Epoch: 0901 cost= 0.613349553\n",
      "Epoch: 0911 cost= 0.613186401\n",
      "Epoch: 0921 cost= 0.613157067\n",
      "Epoch: 0931 cost= 0.612682751\n",
      "Epoch: 0941 cost= 0.612538398\n",
      "Epoch: 0951 cost= 0.612129562\n",
      "Epoch: 0961 cost= 0.611888945\n",
      "Epoch: 0971 cost= 0.611754059\n",
      "Epoch: 0981 cost= 0.611499491\n",
      "Epoch: 0991 cost= 0.611470580\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.608646\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(training_x)/batch_size)\n",
    "        X_batches = np.array_split(training_x, total_batch)\n",
    "        Y_batches = np.array_split(tf_training_y, total_batch)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Testing Accuracy:\", accuracy.eval({x: validation_x, y: tf_validation_y}))\n",
    "    global result \n",
    "    result = tf.argmax(pred, 1).eval({x: validation_x, y: tf_validation_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting submission\n",
    "Submission should be a csv with one column labeled \"instance\" and the other \"tf\" which should contain the prediction of \"T\" or \"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_predictions = random_forest.predict(featurized_testing)\n",
    "predictions = featurized_testing.reset_index()[['instance']]\n",
    "predictions['tf'] = np.array(list(map(lambda x: 'T' if x == 1 else 'F', _predictions)))\n",
    "predictions.to_csv('random_forest_example_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
